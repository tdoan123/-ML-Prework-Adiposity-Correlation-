{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdoan123/-ML-Prework-Adiposity-Correlation-/blob/master/week02_session02_NB04_floating_point.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281e3b5e",
      "metadata": {
        "id": "281e3b5e"
      },
      "source": [
        "_Main topics covered during today's session:_\n",
        "\n",
        "Prior NBs:\n",
        "\n",
        "1. **Counting values in a dictionary**\n",
        "\n",
        "2. **Comparing dictionaries, and finding the highest/lowest value.**\n",
        "\n",
        "3. **Inverting a dictionary**\n",
        "\n",
        "\n",
        "This NB:\n",
        "\n",
        "4. **Teaser -- Floating point values**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40711b9d",
      "metadata": {
        "id": "40711b9d"
      },
      "source": [
        "# Teaser -- Floating Point Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b315d276",
      "metadata": {
        "id": "b315d276"
      },
      "source": [
        "#### We are NOT going to deep dive into how floating values are stored and manipulated in computers in this session. ####\n",
        "\n",
        "Homework Notebook 4 has you recreating this underlying storage paradigm, and we do this for two reasons:\n",
        "\n",
        "1. So that you can understand why floating point arithmetic in computers is not exact,\n",
        "2. And you also understand that it operates within an error bounds that we recognize and accept (or not accept).\n",
        "\n",
        "**What we are going to do here is show you Prof. Vuduc's \"Floating Point Teaser\" introductory video. But first, we are going to deep dive a little into what he covers in the video, so that you have a better understanding, as he is going through it.**\n",
        "\n",
        "**We will be showing Prof. Vuduc's floating point introduction lecture in its entirety next week. It is essentially an extended lecture version of the videos that you have watched for this topic/notebook.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b8b275",
      "metadata": {
        "id": "96b8b275"
      },
      "source": [
        "## The Question for Notebook 4 ##\n",
        "\n",
        "See this [Google Colab notebook](https://colab.research.google.com/drive/1-MlOoW5y2TznOm_LmBjlArjbkwvMrykJ?usp=sharing) which will illustrate how floating point arithmetic is inexact.\n",
        "\n",
        "The notebook uses the torch module in Python, which provides tensor computation (similar to Numpy) using GPU acceleration.\n",
        "\n",
        "We will observe how two computations that should produce identical answers appear to be ever so slightly different.\n",
        "\n",
        "#### The question we pose, then, is this: Are you okay with that? How would you know? ####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6407c0",
      "metadata": {
        "id": "af6407c0"
      },
      "source": [
        "******************************************************************************\n",
        "Below are a couple of links about torch, for your reference:\n",
        "\n",
        "Home page for documentation:  https://pypi.org/project/torch/\n",
        "\n",
        "An example of using torch:  https://www.geeksforgeeks.org/getting-started-with-pytorch/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa6f126",
      "metadata": {
        "id": "0aa6f126"
      },
      "source": [
        "********************************************************************************\n",
        "#### What does the Colab NB do?\n",
        "\n",
        "1. Creates two 1000x1000 tensors (arrays) randomly populated from 0-1 (rand), called a_torch, b_torch. These are standard tensors that use the cpu for operations.\n",
        "2. Performs matrix multiplication of the two tensors (@) to a new tensor -- c_torch\n",
        "3. Does a timing of the matrix multiplication.\n",
        "4. Creates two new tensors as copies of the first two, using the GPU for operations, called a_gpu, b_gpu.  https://www.geeksforgeeks.org/how-to-set-up-and-run-cuda-operations-in-pytorch/ (for a reference on this code)\n",
        "5. Performs the matrix multiplication as in step 2 to a new tensor, c_gpu. Note that this uses the GPU for its operation.\n",
        "6. Does a timing of step 5, and we can see how much faster it is, using the GPU.\n",
        "7. Finds the absolute value of the maximum difference between the cpu and gpu tensors. Intuitively, we would think that there should be no difference between them, but that is not the case.\n",
        "8. Does a timing of the data transfer from cpu to gpu.\n",
        "\n",
        "#### What does this absolute value difference represent?\n",
        "\n",
        "It is the \"floating point error\" because the cpu and gpu tensors stored the same values slightly differently. Remember, the random values were between 0 and 1, so what is the potential magnitude of this error?\n",
        "\n",
        "#### So the question for us is this: Is this error magnitude acceptable?\n",
        "************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25c7365",
      "metadata": {
        "id": "b25c7365"
      },
      "source": [
        "And while this might seem a bit esoteric to us now, this is extremely important in many scientific, medical, and other data-intensive fields. Having a good understanding of these concepts will serve you and your clients well in your future analytics roles.\n",
        "\n",
        "If you decide to take the class MGT 8813 Financial Modeling, there is an ongoing exercise in Excel in which you must decide to either round or truncate to 2 decimals, for the currency, as the rate of return calculations will not compute correctly, if you simply leave the numbers in their native (floating point) format.\n",
        "\n",
        "This might seem a contrived example, but many on the TA Staff have seen this exact scenario, and ones similar to it in other disciplines, in our professional roles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a09cb7d",
      "metadata": {
        "id": "5a09cb7d"
      },
      "source": [
        "### Below are some excellent links to relevant material about this topic. All of these are also linked to on the \"Helpful Resources\" page of the course web site."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6348d72",
      "metadata": {
        "id": "e6348d72"
      },
      "source": [
        "https://www.soa.org/news-and-publications/newsletters/compact/2014/may/com-2014-iss51/losing-my-precision-tips-for-handling-tricky-floating-point-arithmetic/ -- Good article on floating point arithmetic. At the end, it essentially walks through the underlying theory/application of how to solve NB4 Part 1.\n",
        "\n",
        "https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html -- \"What Every Computer Scientist Should Know About Floating-Point Arithmetic\". The title says it all, very good \"next reference\" from the lectures.\n",
        "\n",
        "https://www.h-schmidt.net/FloatConverter/IEEE754.html -- Online floating point converter. Like regex101 for floating point numbers. This one is useful for troubleshooting your code in Notebook 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9fb33a6",
      "metadata": {
        "id": "b9fb33a6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100be7b5",
      "metadata": {
        "id": "100be7b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}